{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "\n",
    "For your final project, you will build a classifer for\n",
    "the **Backorder Prediction** dataset by following our\n",
    "operationalized machine learning pipeline.\n",
    "\n",
    "![AppliedML_Workflow IMAGE MISSING](../images/AppliedML_Workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Data\n",
    "\n",
    "Details of the dataset are located here:\n",
    "\n",
    "Dataset (originally posted on Kaggle): https://www.kaggle.com/tiredgeek/predict-bo-trial\n",
    "\n",
    "The files are accessible in the JupyterHub environment:\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Training_Dataset_v2.csv`\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    "\n",
    "The data is used to predict likelihood of product to go on Back Order.\n",
    " \n",
    "<span style='background:yellow'>**NOTE:** The training data file is 117MB. **Do NOT try to version control any data files** (training, test, or created), you will blow-through the _push limit_.</span>  \n",
    "You can easily lock up a notebook with bad coding practices.  \n",
    "Please save you project early, and often, and use `git commits` to checkpoint your process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration, Training, and Validation\n",
    "\n",
    "You will examine the _training_ dataset and perform \n",
    " * **data preparation and exploratory data analysis**, \n",
    " * **anomaly detection / removal**,\n",
    " * **dimensionality reduction** and then\n",
    " * **train and validate 3 different models**.\n",
    "\n",
    "Of the 3 different models, you are free to pick any estimator from Scikit-Learn \n",
    "or models we have so far covered using TensorFlow.\n",
    "\n",
    "### Validation Assessment\n",
    "\n",
    "Your first, intermediate, result will be an **assessment** of the models' performance.\n",
    "This assessement should be grounded within a 10-fold cross-validation methodology.\n",
    "\n",
    "This should include the confusion matrix and F-score for each classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing\n",
    "\n",
    "Once you have chosen your final model, you will need to re-train it using all the training data.\n",
    "\n",
    "\n",
    "--- \n",
    "##  Overview / Roadmap\n",
    "\n",
    "**General steps**:\n",
    "* Training and Validation\n",
    "  * Dataset carpentry & Exploratory Data Analysis\n",
    "    * Develop functions to perform the necessary steps, you will have to carpentry the Training and the Testing data.\n",
    "  * Generate a **smart sample** of the the data\n",
    "  * Create 3 alternative pipelines, each does:\n",
    "      * Anomaly detection\n",
    "      * Dimensionality reduction\n",
    "      * Model training/validation\n",
    "* Testing\n",
    "  * Train chosen model full training data\n",
    "  * Evaluate model against testing\n",
    "  * Write a summary of your processing and an analysis of the model performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "**Description**\n",
    "~~~\n",
    "sku - Random ID for the product\n",
    "national_inv - Current inventory level for the part\n",
    "lead_time - Transit time for product (if available)\n",
    "in_transit_qty - Amount of product in transit from source\n",
    "forecast_3_month - Forecast sales for the next 3 months\n",
    "forecast_6_month - Forecast sales for the next 6 months\n",
    "forecast_9_month - Forecast sales for the next 9 months\n",
    "sales_1_month - Sales quantity for the prior 1 month time period\n",
    "sales_3_month - Sales quantity for the prior 3 month time period\n",
    "sales_6_month - Sales quantity for the prior 6 month time period\n",
    "sales_9_month - Sales quantity for the prior 9 month time period\n",
    "min_bank - Minimum recommend amount to stock\n",
    "potential_issue - Source issue for part identified\n",
    "pieces_past_due - Parts overdue from source\n",
    "perf_6_month_avg - Source performance for prior 6 month period\n",
    "perf_12_month_avg - Source performance for prior 12 month period\n",
    "local_bo_qty - Amount of stock orders overdue\n",
    "deck_risk - Part risk flag\n",
    "oe_constraint - Part risk flag\n",
    "ppap_risk - Part risk flag\n",
    "stop_auto_buy - Part risk flag\n",
    "rev_stop - Part risk flag\n",
    "went_on_backorder - Product actually went on backorder. \n",
    "~~~\n",
    "\n",
    "**Note**: This is a real-world dataset without any preprocessing.  \n",
    "There will also be warnings due to fact that the 1st column is mixing integer and string values.  \n",
    "**NOTE:** The last column, `went_on_backorder`, is what we are trying to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>sales_9_month</th>\n",
       "      <th>min_bank</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.586967e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "      <td>1.687860e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4.961118e+02</td>\n",
       "      <td>7.872267e+00</td>\n",
       "      <td>4.405202e+01</td>\n",
       "      <td>1.781193e+02</td>\n",
       "      <td>3.449867e+02</td>\n",
       "      <td>5.063644e+02</td>\n",
       "      <td>5.592607e+01</td>\n",
       "      <td>1.750259e+02</td>\n",
       "      <td>3.417288e+02</td>\n",
       "      <td>5.252697e+02</td>\n",
       "      <td>5.277230e+01</td>\n",
       "      <td>2.043724e+00</td>\n",
       "      <td>-6.872059e+00</td>\n",
       "      <td>-6.437947e+00</td>\n",
       "      <td>6.264507e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.961523e+04</td>\n",
       "      <td>7.056024e+00</td>\n",
       "      <td>1.342742e+03</td>\n",
       "      <td>5.026553e+03</td>\n",
       "      <td>9.795152e+03</td>\n",
       "      <td>1.437892e+04</td>\n",
       "      <td>1.928196e+03</td>\n",
       "      <td>5.192378e+03</td>\n",
       "      <td>9.613167e+03</td>\n",
       "      <td>1.483861e+04</td>\n",
       "      <td>1.254983e+03</td>\n",
       "      <td>2.360165e+02</td>\n",
       "      <td>2.655636e+01</td>\n",
       "      <td>2.584333e+01</td>\n",
       "      <td>3.372224e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-2.725600e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.900000e+01</td>\n",
       "      <td>-9.900000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.300000e-01</td>\n",
       "      <td>6.600000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.200000e-01</td>\n",
       "      <td>8.100000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>3.100000e+01</td>\n",
       "      <td>4.700000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.700000e-01</td>\n",
       "      <td>9.500000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.233440e+07</td>\n",
       "      <td>5.200000e+01</td>\n",
       "      <td>4.894080e+05</td>\n",
       "      <td>1.427612e+06</td>\n",
       "      <td>2.461360e+06</td>\n",
       "      <td>3.777304e+06</td>\n",
       "      <td>7.417740e+05</td>\n",
       "      <td>1.105478e+06</td>\n",
       "      <td>2.146625e+06</td>\n",
       "      <td>3.205172e+06</td>\n",
       "      <td>3.133190e+05</td>\n",
       "      <td>1.464960e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.253000e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       national_inv     lead_time  in_transit_qty  forecast_3_month  \\\n",
       "count  1.687860e+06  1.586967e+06    1.687860e+06      1.687860e+06   \n",
       "mean   4.961118e+02  7.872267e+00    4.405202e+01      1.781193e+02   \n",
       "std    2.961523e+04  7.056024e+00    1.342742e+03      5.026553e+03   \n",
       "min   -2.725600e+04  0.000000e+00    0.000000e+00      0.000000e+00   \n",
       "25%    4.000000e+00  4.000000e+00    0.000000e+00      0.000000e+00   \n",
       "50%    1.500000e+01  8.000000e+00    0.000000e+00      0.000000e+00   \n",
       "75%    8.000000e+01  9.000000e+00    0.000000e+00      4.000000e+00   \n",
       "max    1.233440e+07  5.200000e+01    4.894080e+05      1.427612e+06   \n",
       "\n",
       "       forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "count      1.687860e+06      1.687860e+06   1.687860e+06   1.687860e+06   \n",
       "mean       3.449867e+02      5.063644e+02   5.592607e+01   1.750259e+02   \n",
       "std        9.795152e+03      1.437892e+04   1.928196e+03   5.192378e+03   \n",
       "min        0.000000e+00      0.000000e+00   0.000000e+00   0.000000e+00   \n",
       "25%        0.000000e+00      0.000000e+00   0.000000e+00   0.000000e+00   \n",
       "50%        0.000000e+00      0.000000e+00   0.000000e+00   1.000000e+00   \n",
       "75%        1.200000e+01      2.000000e+01   4.000000e+00   1.500000e+01   \n",
       "max        2.461360e+06      3.777304e+06   7.417740e+05   1.105478e+06   \n",
       "\n",
       "       sales_6_month  sales_9_month      min_bank  pieces_past_due  \\\n",
       "count   1.687860e+06   1.687860e+06  1.687860e+06     1.687860e+06   \n",
       "mean    3.417288e+02   5.252697e+02  5.277230e+01     2.043724e+00   \n",
       "std     9.613167e+03   1.483861e+04  1.254983e+03     2.360165e+02   \n",
       "min     0.000000e+00   0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "25%     0.000000e+00   0.000000e+00  0.000000e+00     0.000000e+00   \n",
       "50%     2.000000e+00   4.000000e+00  0.000000e+00     0.000000e+00   \n",
       "75%     3.100000e+01   4.700000e+01  3.000000e+00     0.000000e+00   \n",
       "max     2.146625e+06   3.205172e+06  3.133190e+05     1.464960e+05   \n",
       "\n",
       "       perf_6_month_avg  perf_12_month_avg  local_bo_qty  \n",
       "count      1.687860e+06       1.687860e+06  1.687860e+06  \n",
       "mean      -6.872059e+00      -6.437947e+00  6.264507e-01  \n",
       "std        2.655636e+01       2.584333e+01  3.372224e+01  \n",
       "min       -9.900000e+01      -9.900000e+01  0.000000e+00  \n",
       "25%        6.300000e-01       6.600000e-01  0.000000e+00  \n",
       "50%        8.200000e-01       8.100000e-01  0.000000e+00  \n",
       "75%        9.700000e-01       9.500000e-01  0.000000e+00  \n",
       "max        1.000000e+00       1.000000e+00  1.253000e+04  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Training_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "In this section, the goal is to figure out:\n",
    "\n",
    "* which columns we can use directly,  \n",
    "* which columns are usable after some processing,  \n",
    "* and which columns are not processable or obviously irrelevant (like product id) that we will discard.\n",
    "\n",
    "Then process and prepare this dataset for creating a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take samples and examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1793815</td>\n",
       "      <td>247.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1697019</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1239403</td>\n",
       "      <td>140.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  1793815         247.0        4.0             0.0              81.0   \n",
       "1  1697019           5.0        8.0             0.0               0.0   \n",
       "2  1239403         140.0       15.0             0.0               0.0   \n",
       "\n",
       "   forecast_6_month  \n",
       "0              81.0  \n",
       "1               0.0  \n",
       "2             100.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:3,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>sales_9_month</th>\n",
       "      <th>min_bank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   forecast_9_month  sales_1_month  sales_3_month  sales_6_month  \\\n",
       "0              81.0           32.0          148.0          396.0   \n",
       "1               0.0            0.0            0.0            0.0   \n",
       "2             100.0           11.0           36.0           85.0   \n",
       "\n",
       "   sales_9_month  min_bank  \n",
       "0          709.0       0.0  \n",
       "1            0.0       0.0  \n",
       "2          151.0      16.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:3,6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>potential_issue</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  potential_issue  pieces_past_due  perf_6_month_avg  perf_12_month_avg  \\\n",
       "0              No              0.0              0.73               0.78   \n",
       "1              No              0.0              0.82               0.75   \n",
       "2              No              0.0              0.50               0.44   \n",
       "\n",
       "   local_bo_qty deck_risk  \n",
       "0           0.0        No  \n",
       "1           0.0        No  \n",
       "2           0.0        No  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:3,12:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  oe_constraint ppap_risk stop_auto_buy rev_stop went_on_backorder\n",
       "0            No        No           Yes       No                No\n",
       "1            No        No           Yes       No                No\n",
       "2            No        No           Yes       No                No"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:3,18:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns that are obviously irrelevant or not processable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8001)\n",
    "# ----------------------------------\n",
    "\n",
    "#dataset.del['sku']\n",
    "dataset=dataset.drop('sku', axis=1)\n",
    "#dataset=dataset.drop('AppointmentID', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique values of string columns\n",
    "\n",
    "Now try to make sure that these Yes/No columns really only contains Yes or No.  \n",
    "If that's true, proceed to convert them into binaries (0s and 1s).\n",
    "\n",
    "**Tip**: use [unique()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) function of pandas Series.\n",
    "\n",
    "Example\n",
    "\n",
    "~~~python\n",
    "print('went_on_backorder', dataset['went_on_backorder'].unique())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "'potential_issue: ' ['No' 'Yes' nan]\n",
      "'deck_risk: ' ['No' 'Yes' nan]\n",
      "'oe_constraint: ' ['No' 'Yes' nan]\n",
      "'ppap_risk: ' ['No' 'Yes' nan]\n",
      "'stop_auto_buy: ' ['Yes' 'No' nan]\n",
      "'rev_stop: ' ['No' 'Yes' nan]\n",
      "'went_on_backorder: ' ['No' 'Yes' nan]\n"
     ]
    }
   ],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E8002)\n",
    "# ----------------------------------\n",
    "def PrintUniqueValues(yes_no_column,dataset):\n",
    "    for i in yes_no_column:\n",
    "        print(\"'\"+i+\": '\", dataset[i].unique())\n",
    "PrintUniqueValues(yes_no_columns,dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see **nan** also as possible values representing missing values in the dataset.\n",
    "\n",
    "We fill them using most popular values, the [Mode](https://en.wikipedia.org/wiki/Mode_%28statistics%29) in Stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert yes/no columns into binary (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'potential_issue: ' [1 0]\n",
      "'deck_risk: ' [1 0]\n",
      "'oe_constraint: ' [1 0]\n",
      "'ppap_risk: ' [1 0]\n",
      "'stop_auto_buy: ' [0 1]\n",
      "'rev_stop: ' [1 0]\n",
      "'went_on_backorder: ' [1 0]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1687861 entries, 0 to 1687860\n",
      "Data columns (total 22 columns):\n",
      "national_inv         1687860 non-null float64\n",
      "lead_time            1586967 non-null float64\n",
      "in_transit_qty       1687860 non-null float64\n",
      "forecast_3_month     1687860 non-null float64\n",
      "forecast_6_month     1687860 non-null float64\n",
      "forecast_9_month     1687860 non-null float64\n",
      "sales_1_month        1687860 non-null float64\n",
      "sales_3_month        1687860 non-null float64\n",
      "sales_6_month        1687860 non-null float64\n",
      "sales_9_month        1687860 non-null float64\n",
      "min_bank             1687860 non-null float64\n",
      "potential_issue      1687861 non-null int64\n",
      "pieces_past_due      1687860 non-null float64\n",
      "perf_6_month_avg     1687860 non-null float64\n",
      "perf_12_month_avg    1687860 non-null float64\n",
      "local_bo_qty         1687860 non-null float64\n",
      "deck_risk            1687861 non-null int64\n",
      "oe_constraint        1687861 non-null int64\n",
      "ppap_risk            1687861 non-null int64\n",
      "stop_auto_buy        1687861 non-null int64\n",
      "rev_stop             1687861 non-null int64\n",
      "went_on_backorder    1687861 non-null int64\n",
      "dtypes: float64(15), int64(7)\n",
      "memory usage: 283.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E8003)\n",
    "# ----------------------------------\n",
    "def CovertToBinary(yes_no_column,data):\n",
    "    for i in yes_no_column:\n",
    "        data[i] = data[i].apply(['Yes', 'No'].index)\n",
    "\n",
    "CovertToBinary(yes_no_columns,dataset)\n",
    "PrintUniqueValues(yes_no_columns,dataset)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all columns should be either int64 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cleaning the dataset for anymore NAN or long float values, or negative values.\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "\n",
    "def delete_negative_values(X_data):\n",
    "    for i in X_data.columns:\n",
    "        if any(X_data[i]<0):\n",
    "            X_data = X_data[X_data[i] > 0]\n",
    "            X_data = X_data.dropna()\n",
    "            print(any(X_data[i]<0))\n",
    "    return X_data\n",
    "\n",
    "def replace_negative_values_with_0(X_data):\n",
    "    for i in X_data.columns:\n",
    "        if any(X_data[i]<0):\n",
    "            X_data[X_data[i]<0]=0 #a[a < 0] = 0\n",
    "            print(any(X_data[i]<0))\n",
    "    return X_data\n",
    "#dataset=delete_negative_values(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1586967, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First Cleaning the dataset dealing with NAN or long float values\n",
    "clean_dataset(dataset)\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform additional steps to smartly sample the data into a more manageable size for cross-fold validation\n",
    "\n",
    "**Note:** After sampling the data, you may want to write the data to a file for reloading later.\n",
    "Remove the old `dataset` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment   (Question #E8004) \n",
    "# ----------------------------------\n",
    "\n",
    "#Resampling after cleaning the data on NAN or long float values.\n",
    "dataset_resampled = pd.concat([\n",
    "    dataset[dataset['went_on_backorder'] == 1].sample(frac = 1).reset_index(drop=True),\n",
    "    dataset[dataset['went_on_backorder'] == 0]\n",
    "])\n",
    "\n",
    "dataset_resampled = dataset_resampled.sample(frac = 1).reset_index(drop=True)\n",
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went_on_backorder ratio: index                1.330102e+12\n",
      "national_inv         7.766043e+08\n",
      "lead_time            1.242368e+07\n",
      "in_transit_qty       7.212486e+07\n",
      "forecast_3_month     2.977890e+08\n",
      "forecast_6_month     5.770620e+08\n",
      "forecast_9_month     8.474303e+08\n",
      "sales_1_month        9.001331e+07\n",
      "sales_3_month        2.824282e+08\n",
      "sales_6_month        5.575383e+08\n",
      "sales_9_month        8.613777e+08\n",
      "min_bank             8.417925e+07\n",
      "potential_issue      1.575147e+06\n",
      "pieces_past_due      3.408234e+06\n",
      "perf_6_month_avg    -1.607801e+06\n",
      "perf_12_month_avg   -8.785444e+05\n",
      "local_bo_qty         9.543310e+05\n",
      "deck_risk            1.255292e+06\n",
      "oe_constraint        1.575761e+06\n",
      "ppap_risk            1.389139e+06\n",
      "stop_auto_buy        3.877000e+04\n",
      "rev_stop             1.575588e+06\n",
      "went_on_backorder    1.575998e+06\n",
      "dtype: float64 / 1586967 = index                838140.622366\n",
      "national_inv            489.363844\n",
      "lead_time                 7.828566\n",
      "in_transit_qty           45.448244\n",
      "forecast_3_month        187.646596\n",
      "forecast_6_month        363.625709\n",
      "forecast_9_month        533.993651\n",
      "sales_1_month            56.720341\n",
      "sales_3_month           177.967301\n",
      "sales_6_month           351.323178\n",
      "sales_9_month           542.782343\n",
      "min_bank                 53.044107\n",
      "potential_issue           0.992552\n",
      "pieces_past_due           2.147640\n",
      "perf_6_month_avg         -1.013128\n",
      "perf_12_month_avg        -0.553600\n",
      "local_bo_qty              0.601355\n",
      "deck_risk                 0.791001\n",
      "oe_constraint             0.992939\n",
      "ppap_risk                 0.875342\n",
      "stop_auto_buy             0.024430\n",
      "rev_stop                  0.992830\n",
      "went_on_backorder         0.993088\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "num_went_on_backorder = np.sum(dataset[dataset['went_on_backorder']==1]) # find out total number of no-show cases\n",
    "print('went_on_backorder ratio:', num_went_on_backorder, '/', len(dataset), '=', num_went_on_backorder / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsample_rate: index                -0.999999\n",
      "national_inv         -0.997957\n",
      "lead_time            -0.872263\n",
      "in_transit_qty       -0.977997\n",
      "forecast_3_month     -0.994671\n",
      "forecast_6_month     -0.997250\n",
      "forecast_9_month     -0.998127\n",
      "sales_1_month        -0.982370\n",
      "sales_3_month        -0.994381\n",
      "sales_6_month        -0.997154\n",
      "sales_9_month        -0.998158\n",
      "min_bank             -0.981148\n",
      "potential_issue       0.007504\n",
      "pieces_past_due      -0.534373\n",
      "perf_6_month_avg     -1.987042\n",
      "perf_12_month_avg    -2.806360\n",
      "local_bo_qty          0.662910\n",
      "deck_risk             0.264221\n",
      "oe_constraint         0.007111\n",
      "ppap_risk             0.142411\n",
      "stop_auto_buy        39.932860\n",
      "rev_stop              0.007222\n",
      "went_on_backorder     0.006960\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#went_on_backorder\n",
    "upsample_rate = (len(dataset) - num_went_on_backorder) / num_went_on_backorder\n",
    "print('upsample_rate:', upsample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                1586967\n",
      "national_inv         1586967\n",
      "lead_time            1586967\n",
      "in_transit_qty       1586967\n",
      "forecast_3_month     1586967\n",
      "forecast_6_month     1586967\n",
      "forecast_9_month     1586967\n",
      "sales_1_month        1586967\n",
      "sales_3_month        1586967\n",
      "sales_6_month        1586967\n",
      "sales_9_month        1586967\n",
      "min_bank             1586967\n",
      "potential_issue      1586967\n",
      "pieces_past_due      1586967\n",
      "perf_6_month_avg     1586967\n",
      "perf_12_month_avg    1586967\n",
      "local_bo_qty         1586967\n",
      "deck_risk            1586967\n",
      "oe_constraint        1586967\n",
      "ppap_risk            1586967\n",
      "stop_auto_buy        1586967\n",
      "rev_stop             1586967\n",
      "went_on_backorder    1586967\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index                26729\n",
       "national_inv         26729\n",
       "lead_time            26729\n",
       "in_transit_qty       26729\n",
       "forecast_3_month     26729\n",
       "forecast_6_month     26729\n",
       "forecast_9_month     26729\n",
       "sales_1_month        26729\n",
       "sales_3_month        26729\n",
       "sales_6_month        26729\n",
       "sales_9_month        26729\n",
       "min_bank             26729\n",
       "potential_issue      26729\n",
       "pieces_past_due      26729\n",
       "perf_6_month_avg     26729\n",
       "perf_12_month_avg    26729\n",
       "local_bo_qty         26729\n",
       "deck_risk            26729\n",
       "oe_constraint        26729\n",
       "ppap_risk            26729\n",
       "stop_auto_buy        26729\n",
       "rev_stop             26729\n",
       "went_on_backorder    26729\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_resampled = pd.concat([\n",
    "    dataset[dataset['went_on_backorder'] == 1].sample(frac = 0.01).reset_index(drop=True),\n",
    "    dataset[dataset['went_on_backorder'] == 0]\n",
    "])\n",
    "print(dataset.count())\n",
    "dataset_resampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                8019\n",
       "national_inv         8019\n",
       "lead_time            8019\n",
       "in_transit_qty       8019\n",
       "forecast_3_month     8019\n",
       "forecast_6_month     8019\n",
       "forecast_9_month     8019\n",
       "sales_1_month        8019\n",
       "sales_3_month        8019\n",
       "sales_6_month        8019\n",
       "sales_9_month        8019\n",
       "min_bank             8019\n",
       "potential_issue      8019\n",
       "pieces_past_due      8019\n",
       "perf_6_month_avg     8019\n",
       "perf_12_month_avg    8019\n",
       "local_bo_qty         8019\n",
       "deck_risk            8019\n",
       "oe_constraint        8019\n",
       "ppap_risk            8019\n",
       "stop_auto_buy        8019\n",
       "rev_stop             8019\n",
       "went_on_backorder    8019\n",
       "dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_resampled\n",
    "\n",
    "# Load and shuffle\n",
    "dataset_resampled = dataset_resampled.sample(frac = 0.3).reset_index(drop=True)\n",
    "#dataset_resampled.describe()\n",
    "#\n",
    "dataset_resampled.head()\n",
    "dataset_resampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went_on_backorder ratio: 0.5874797356278838\n",
      "went_on_backorder ratio: 0.993088073034915\n"
     ]
    }
   ],
   "source": [
    "print('went_on_backorder ratio:', np.sum(dataset_resampled['went_on_backorder'] == 1) / len(dataset_resampled))\n",
    "print('went_on_backorder ratio:', np.sum(dataset['went_on_backorder'] == 1) / len(dataset))\n",
    "#went_on_backorder ratio: 0.5936845255911404 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smarty resampled data \n",
    "\n",
    "Samrtly re-sampled data had a went_on_backorder ratio: 0.5936845255911404, which looked quite balanced subset of the original dataset. Original dataset had a went_on_backorder ratio: 0.993088073034915."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your smart sampling to local file  (Question #E8004 ... cont. ) \n",
    "# ----------------------------------\n",
    "dataset_resampled.to_csv('sampled2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a couple commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Data Sampled`\n",
    "\n",
    "### <center><span style='color:green'>This becomes the new Starting Point after initial data work</span></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 23)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Reload your smart sampling from local file  (Question #E8004 ... cont.) \n",
    "# ----------------------------------\n",
    "\n",
    "RESAMPLED = 'sampled2.csv'\n",
    "assert os.path.exists(RESAMPLED)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset2 = pd.read_csv(RESAMPLED).sample(frac = 1).reset_index(drop=True)\n",
    "dataset2.describe()\n",
    "\n",
    "dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13364, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the subset into X and y\n",
    "\n",
    "X=dataset2.iloc[:,:-1]\n",
    "y=dataset2.went_on_backorder\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes (X, y):  (12027, 22) (12027,)\n",
      "Testing shapes (X, y):  (1337, 22) (1337,)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(\"Training shapes (X, y): \", X_train.shape, y_train.shape)\n",
    "print(\"Testing shapes (X, y): \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "X_test2=replace_negative_values_with_0(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this section, design an operationalized machine learning pipeline, which includes:\n",
    "\n",
    "* Anomaly detection\n",
    "* Dimensionality Reduction\n",
    "* Train a model\n",
    "\n",
    "**Note:** <span style='background:yellow'>Ensure you are using Grid Search to find optimal parameters of your pipelines.</span>\n",
    "\n",
    "You can add more notebook cells or import any Python modules as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Red'>Your 1st pipeline </font>\n",
    "  * Anomaly detection - <font color='Red'>SVM method used</font>\n",
    "  * Dimensionality reduction - <font color='Red'>PCA() and selectKBest()</font>\n",
    "  * Model training/validation - <font color='Red'>Ridge()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8005)\n",
    "# ----------------------------------\n",
    "\n",
    "svm = OneClassSVM(kernel='rbf',gamma='auto').fit(X_train, y_train)\n",
    "svm_outliers = svm.predict(X_train)==-1\n",
    "\n",
    "# Pull inliers\n",
    "X_svm = X_train[~svm_outliers] \n",
    "y_svm = y_train[~svm_outliers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM was really hard on removing the ouliers. The subset is slashed down to almost half the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (12027, 22) X_svm.shape: (4329, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \",X_train.shape, \"X_svm.shape:\", X_svm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing negative values in data\n",
    "The followng step is to replace the -ve values with zero because without this step the pipeline was failing saying \"can't handle the negative values.\" I tried first deleting the negative values, but then a lot of data was going away. Hence I just chose replacing negative values with 0 instead of dropping them.\n",
    "\n",
    "Also, I purposefully did it after ouliers removal step because I though ouliers would have been affected if I would have done this step before the anomaly detection. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "X_svm2=replace_negative_values_with_0(X_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params=None, iid='warn', n_jobs=2,\n",
       "       param_grid=[{'reduce_dim__n_components': [2, 4, 5, 6, 10, 12, 18]}, {'reduce_dim': [SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>)], 'reduce_dim__k': [2, 4, 5, 6, 10, 12, 18]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1ST Pipeline with PCA and Ridge\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classifier', Ridge())\n",
    "])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression,mutual_info_regression\n",
    "\n",
    "N_FEATURES_OPTIONS = [2,4,5,6,10,12,18]\n",
    "param_grid = [{\n",
    "    'reduce_dim__n_components': N_FEATURES_OPTIONS\n",
    "},\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(f_regression)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA','KBest(chi2)'] #\n",
    "\n",
    "clf2 = GridSearchCV(pipe,cv=10, n_jobs=2, param_grid=param_grid)\n",
    "\n",
    "clf2.fit(X_svm2, y_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[123 417]\n",
      " [ 69 728]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.23      0.34       540\n",
      "           1       0.64      0.91      0.75       797\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1337\n",
      "   macro avg       0.64      0.57      0.54      1337\n",
      "weighted avg       0.64      0.64      0.58      1337\n",
      "\n",
      "Best Accuracy Score:===>  0.04214284499357722\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf2.predict(X_test2).round()\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", clf2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.13946404, 0.19324427, 0.12543812, 0.14788818, 0.19117432,\n",
       "        0.28480551, 0.09613636, 0.03657842, 0.04688015, 0.02402456,\n",
       "        0.02375894, 0.04015844, 0.03928351, 0.02968147]),\n",
       " 'std_fit_time': array([0.0759725 , 0.11147323, 0.05799941, 0.06367585, 0.05210349,\n",
       "        0.06669843, 0.00433948, 0.03725929, 0.04220813, 0.02974793,\n",
       "        0.02942295, 0.03796989, 0.03945764, 0.03434674]),\n",
       " 'mean_score_time': array([0.00153742, 0.00202346, 0.00208526, 0.00172095, 0.00995138,\n",
       "        0.00216825, 0.00187285, 0.00690937, 0.00152936, 0.00152485,\n",
       "        0.00148675, 0.00978835, 0.00133595, 0.00127347]),\n",
       " 'std_score_time': array([6.74347572e-04, 9.66293265e-04, 9.24548792e-04, 6.04504407e-04,\n",
       "        2.32406592e-02, 9.87240087e-04, 6.80614867e-04, 1.65097738e-02,\n",
       "        3.56327047e-04, 2.91750099e-04, 2.63384112e-04, 2.37231139e-02,\n",
       "        1.81391299e-04, 5.39484982e-05]),\n",
       " 'param_reduce_dim__n_components': masked_array(data=[2, 4, 5, 6, 10, 12, 18, --, --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False,  True,\n",
       "                     True,  True,  True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim': masked_array(data=[--, --, --, --, --, --, --,\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>)],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim__k': masked_array(data=[--, --, --, --, --, --, --, 2, 4, 5, 6, 10, 12, 18],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'reduce_dim__n_components': 2},\n",
       "  {'reduce_dim__n_components': 4},\n",
       "  {'reduce_dim__n_components': 5},\n",
       "  {'reduce_dim__n_components': 6},\n",
       "  {'reduce_dim__n_components': 10},\n",
       "  {'reduce_dim__n_components': 12},\n",
       "  {'reduce_dim__n_components': 18},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 2},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 4},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 5},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 6},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 10},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 12},\n",
       "  {'reduce_dim': SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 18}],\n",
       " 'split0_test_score': array([-0.00282344, -0.0041346 ,  0.01066244,  0.01408957, -0.22610604,\n",
       "        -0.22288524, -0.15030361,  0.03079248,  0.03515342,  0.03212207,\n",
       "         0.03331671,  0.03520337,  0.03433691, -0.261968  ]),\n",
       " 'split1_test_score': array([0.00700519, 0.00712618, 0.01855595, 0.02175338, 0.02719941,\n",
       "        0.02510101, 0.07912755, 0.06046334, 0.05988648, 0.05877332,\n",
       "        0.055135  , 0.06182784, 0.06502838, 0.07377762]),\n",
       " 'split2_test_score': array([ 0.00689197,  0.0029563 , -0.02471583, -0.02123876, -0.03324195,\n",
       "        -0.02631608,  0.01577034,  0.02562205,  0.02793295,  0.02739513,\n",
       "         0.03087424,  0.0426537 ,  0.04563627,  0.05366017]),\n",
       " 'split3_test_score': array([-0.00805426, -0.00727395, -0.08783637, -0.06042354, -0.23787234,\n",
       "        -0.23380742, -0.17156148,  0.04108554,  0.04025423,  0.04006087,\n",
       "         0.03926921, -0.08752868, -0.07448427, -0.372921  ]),\n",
       " 'split4_test_score': array([-0.00430754, -0.00361609,  0.00248541,  0.00557303,  0.00655387,\n",
       "         0.008643  , -0.11130106,  0.06346638,  0.06534124,  0.06684449,\n",
       "         0.06955447, -0.21223548, -0.27507554, -0.16369518]),\n",
       " 'split5_test_score': array([-0.00827739, -0.00702949, -0.00046717,  0.002799  ,  0.00746651,\n",
       "         0.00483702,  0.03293732,  0.0018866 ,  0.01450697,  0.01506712,\n",
       "         0.01582193,  0.0233159 ,  0.02722567,  0.01650778]),\n",
       " 'split6_test_score': array([ 0.00208352, -0.01304033,  0.00875   ,  0.012056  , -0.02852737,\n",
       "        -0.15666714, -0.1030725 ,  0.00401886,  0.02729166,  0.02767921,\n",
       "         0.02814703,  0.03821842,  0.03954672, -0.00272655]),\n",
       " 'split7_test_score': array([-0.00183824, -0.00160519,  0.00762615,  0.01437629,  0.02714148,\n",
       "         0.02582352,  0.07410502,  0.05241008,  0.05444271,  0.05438495,\n",
       "         0.05425996,  0.06049898,  0.06190749,  0.07282045]),\n",
       " 'split8_test_score': array([9.54186456e-05, 6.47005560e-04, 8.13053022e-03, 7.93727791e-03,\n",
       "        1.24337621e-02, 1.34824300e-02, 7.29408450e-02, 5.54354603e-02,\n",
       "        5.66926358e-02, 5.68299546e-02, 6.06091778e-02, 6.59496431e-02,\n",
       "        6.64507638e-02, 7.20847841e-02]),\n",
       " 'split9_test_score': array([ 8.10240934e-03, -2.04278840e-01, -6.41417197e-01, -6.71178292e-01,\n",
       "        -2.15817989e+01, -2.09309896e+01, -1.78438756e+01,  3.43262934e-02,\n",
       "         3.47005663e-02,  3.45333689e-02,  3.44228903e-02, -4.80502193e-02,\n",
       "        -4.19524214e-02, -1.40378812e+01]),\n",
       " 'mean_test_score': array([-1.14134214e-04, -2.29830305e-02, -6.96905707e-02, -6.72861379e-02,\n",
       "        -2.19819857e+00, -2.14493927e+00, -1.80681961e+00,  3.69513125e-02,\n",
       "         4.16218841e-02,  4.13706266e-02,  4.21428450e-02, -2.00401838e-03,\n",
       "        -5.12950024e-03, -1.45212747e+00]),\n",
       " 'std_test_score': array([5.74158137e-03, 6.06020116e-02, 1.92610057e-01, 2.02360680e-01,\n",
       "        6.45442724e+00, 6.25554989e+00, 5.34031298e+00, 2.08370709e-02,\n",
       "        1.58358769e-02, 1.60289221e-02, 1.60308783e-02, 8.47821720e-02,\n",
       "        1.00386464e-01, 4.19308153e+00]),\n",
       " 'rank_test_score': array([ 5,  8, 10,  9, 14, 13, 12,  4,  2,  3,  1,  6,  7, 11],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([0.00394517, 0.0042795 , 0.01157484, 0.01410961, 0.02393013,\n",
       "        0.02418974, 0.06881485, 0.04641645, 0.04671477, 0.04924969,\n",
       "        0.04954496, 0.05535016, 0.0557734 , 0.06815251]),\n",
       " 'split1_train_score': array([0.00300837, 0.00350954, 0.01095393, 0.01474164, 0.01969405,\n",
       "        0.02028226, 0.06335786, 0.04328021, 0.04426587, 0.04479367,\n",
       "        0.04747821, 0.05277567, 0.05443421, 0.06092653]),\n",
       " 'split2_train_score': array([0.00246719, 0.00287405, 0.0125564 , 0.01283486, 0.01797952,\n",
       "        0.01830725, 0.06546844, 0.04672385, 0.04738431, 0.04784489,\n",
       "        0.04917648, 0.05452851, 0.05605192, 0.06116003]),\n",
       " 'split3_train_score': array([0.00341323, 0.00371269, 0.01620516, 0.01782289, 0.02492002,\n",
       "        0.02559187, 0.06834166, 0.04420108, 0.04521301, 0.04562119,\n",
       "        0.04803075, 0.05589941, 0.05799661, 0.06859796]),\n",
       " 'split4_train_score': array([0.00424579, 0.00467406, 0.01275221, 0.01654364, 0.02204148,\n",
       "        0.02214641, 0.06543967, 0.0429494 , 0.04366415, 0.0437456 ,\n",
       "        0.04530117, 0.05522637, 0.05696169, 0.06167925]),\n",
       " 'split5_train_score': array([0.00389653, 0.00425824, 0.01227493, 0.01606162, 0.02104738,\n",
       "        0.02154154, 0.06733179, 0.02676358, 0.04834928, 0.04867385,\n",
       "        0.05041379, 0.05610399, 0.05781911, 0.06451028]),\n",
       " 'split6_train_score': array([0.00328875, 0.00745827, 0.01137883, 0.01505456, 0.02346149,\n",
       "        0.02558015, 0.07056328, 0.02710624, 0.04751362, 0.0478535 ,\n",
       "        0.04962119, 0.05501667, 0.05671361, 0.06353041]),\n",
       " 'split7_train_score': array([0.00394405, 0.00442409, 0.01213574, 0.01546262, 0.01967634,\n",
       "        0.02022642, 0.06390169, 0.04416242, 0.04486342, 0.04525428,\n",
       "        0.04708903, 0.05291494, 0.05491428, 0.06079453]),\n",
       " 'split8_train_score': array([0.00380875, 0.0042765 , 0.01214621, 0.01630099, 0.02127087,\n",
       "        0.02157249, 0.06410311, 0.04388895, 0.04462817, 0.04500874,\n",
       "        0.04629906, 0.05221674, 0.05431504, 0.0599756 ]),\n",
       " 'split9_train_score': array([0.00239709, 0.01021096, 0.01479073, 0.0148025 , 0.02255653,\n",
       "        0.02297796, 0.06831443, 0.04623762, 0.04712076, 0.04752435,\n",
       "        0.04935666, 0.05668455, 0.05685275, 0.06574388]),\n",
       " 'mean_train_score': array([0.00344149, 0.00496779, 0.0126769 , 0.01537349, 0.02165778,\n",
       "        0.02224161, 0.06656368, 0.04117298, 0.04597174, 0.04655698,\n",
       "        0.04823113, 0.0546717 , 0.05618326, 0.0635071 ]),\n",
       " 'std_train_score': array([0.00061212, 0.00208844, 0.00153432, 0.00132588, 0.00204569,\n",
       "        0.00225906, 0.00231866, 0.00722869, 0.00154084, 0.00178858,\n",
       "        0.00157518, 0.00145691, 0.00125079, 0.0029807 ])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline grid search.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E8006)\n",
    "# ---------------------------------------------\n",
    "The accuracy for this model was quite low 0.04214. Here is the confusion matrix and f1 score.\n",
    "Confusion Matrix:===>  \n",
    " [[123 417]\n",
    " [ 69 728]] \n",
    "\n",
    "Classification Report:===>  \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.64      0.23      0.34       540\n",
    "           1       0.64      0.91      0.75       797\n",
    "\n",
    "   micro avg       0.64      0.64      0.64      1337\n",
    "   macro avg       0.64      0.57      0.54      1337\n",
    "weighted avg       0.64      0.64      0.58      1337\n",
    "\n",
    "Best Accuracy Score:===>  0.04214284499357722\n",
    "\n",
    "Optimal Hyperparameters:\n",
    "SelectKBest was chosen for the dimension reduction. SelectKbest method was set to f_regression and a variety of n_components were given as N_FEATURES_OPTIONS = [2,5,8,12,18,20] to be run. k=6 was selected as the optimal one. \n",
    "\n",
    "Ridge() clasiifier was chosen.\n",
    "\n",
    "There were total 14 models built up using the hyperparameters and the 5th models gave the best results with accuracy mean accuracy as 0.04214(which is quite low but highest among the models chosen in this pipeline). \n",
    "\n",
    "Here are the mean scores from the 10 cross-folds, 14 models grid:\n",
    "'mean_test_score': array([-1.14134214e-04, -2.29830305e-02, -6.96905707e-02, -6.72861379e-02,\n",
    "        -2.19819857e+00, -2.14493927e+00, -1.80681961e+00,  3.69513125e-02,\n",
    "         4.16218841e-02,  4.13706266e-02,  4.21428450e-02, -2.00401838e-03,\n",
    "        -5.12950024e-03, -1.45212747e+00]),\n",
    "'rank_test_score': array([ 5,  8, 10,  9, 14, 13, 12,  4,  2,  3,  1,  6,  7, 11],\n",
    "       dtype=int32),\n",
    "       \n",
    "Following were the chosen hyperparameters for the optimal pipeline:\n",
    "Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', SelectKBest(k=6, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "   normalize=False, random_state=None, solver='auto', tol=0.001))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Red'>Your 2nd pipeline </font>\n",
    "  * Anomaly detection - <font color='Red'>Isolation Method method used</font>\n",
    "  * Dimensionality reduction - <font color='Red'>PCA() and selectKBest()</font>\n",
    "  * Model training/validation - <font color='Red'>RandomForestClassifier(n_estimators=100, max_depth=10)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:213: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:223: FutureWarning: behaviour=\"old\" is deprecated and will be removed in version 0.22. Please use behaviour=\"new\", which makes the decision_function change to match other anomaly detection algorithm API.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/iforest.py:417: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E8007)\n",
    "# ----------------------------------\n",
    "\n",
    "# Pipeline 2 => Anomaly detection => Construct IsolationForest \n",
    "iso_forest = IsolationForest(n_estimators=250,\n",
    "                             bootstrap=True).fit(X_train, y_train)\n",
    "\n",
    "iso_outliers = iso_forest.predict(X_train)==-1\n",
    "\n",
    "X_iso = X_train[~iso_outliers]\n",
    "y_iso = y_train[~iso_outliers]\n",
    "\n",
    "X_iso2=replace_negative_values_with_0(X_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking at the shape of original training dataset and training dataset without outliers, looks like isolation Forest was not as brutual as svm in removing the ouliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (12027, 22) X_iso.shape: (10824, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \",X_train.shape, \"X_iso.shape:\", X_iso.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets pickle out the anomaly detection classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iso_forest.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(iso_forest, 'iso_forest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=Non...mators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=2,\n",
       "       param_grid=[{'reduce_dim__n_components': [2, 5, 8, 12, 18, 20]}, {'reduce_dim': [SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>)], 'reduce_dim__k': [2, 5, 8, 12, 18, 20]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline with PCA and Random forest classifier\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10,random_state=1))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "N_FEATURES_OPTIONS = [2,5,8,12,18,20]\n",
    "param_grid = [{\n",
    "    'reduce_dim__n_components': N_FEATURES_OPTIONS\n",
    "},\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(f_regression)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS\n",
    "    },\n",
    "]\n",
    "reducer_labels = ['PCA','KBest(chi2)'] #\n",
    "\n",
    "clf3 = GridSearchCV(pipe,cv=10, n_jobs=2, param_grid=param_grid)\n",
    "\n",
    "clf3.fit(X_iso2, y_iso)\n",
    "\n",
    "#n_estimators=500, max_depth=5,random_state=0) 85% x_svm2\n",
    "#n_estimators=100, max_depth=10,random_state=0) 86% x_svm2\n",
    "#n_estimators=100, max_depth=10,random_state=1) 86.6 x_svm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[470  70]\n",
      " [ 92 705]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       540\n",
      "           1       0.91      0.88      0.90       797\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      1337\n",
      "   macro avg       0.87      0.88      0.87      1337\n",
      "weighted avg       0.88      0.88      0.88      1337\n",
      "\n",
      "Best Accuracy Score:===>  0.8732446415373245\n"
     ]
    }
   ],
   "source": [
    "y_pred3=clf3.predict(X_test2).round()\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test, y_pred3),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test, y_pred3))\n",
    "print(\"Best Accuracy Score:===> \", clf3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_im...mators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.45658276, 2.01593521, 2.36447291, 3.03860893, 2.91751575,\n",
       "        3.11890912, 0.31489751, 0.51972735, 0.52162497, 0.73051088,\n",
       "        0.99146113, 0.93290198]),\n",
       " 'std_fit_time': array([0.15329191, 0.16042788, 0.29098683, 0.30247709, 0.17292014,\n",
       "        0.09337753, 0.04683821, 0.05869908, 0.0326775 , 0.03888426,\n",
       "        0.05246475, 0.04226542]),\n",
       " 'mean_score_time': array([0.04450729, 0.03650155, 0.04070563, 0.05709233, 0.07172623,\n",
       "        0.04302967, 0.016027  , 0.02290478, 0.02109427, 0.02491107,\n",
       "        0.02405984, 0.02345941]),\n",
       " 'std_score_time': array([0.04590495, 0.02243414, 0.02933746, 0.03452093, 0.05291311,\n",
       "        0.01331274, 0.00478787, 0.00454521, 0.00066332, 0.0054267 ,\n",
       "        0.00487581, 0.00337116]),\n",
       " 'param_reduce_dim__n_components': masked_array(data=[2, 5, 8, 12, 18, 20, --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False,  True,  True,\n",
       "                     True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim': masked_array(data=[--, --, --, --, --, --,\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>)],\n",
       "              mask=[ True,  True,  True,  True,  True,  True, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim__k': masked_array(data=[--, --, --, --, --, --, 2, 5, 8, 12, 18, 20],\n",
       "              mask=[ True,  True,  True,  True,  True,  True, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'reduce_dim__n_components': 2},\n",
       "  {'reduce_dim__n_components': 5},\n",
       "  {'reduce_dim__n_components': 8},\n",
       "  {'reduce_dim__n_components': 12},\n",
       "  {'reduce_dim__n_components': 18},\n",
       "  {'reduce_dim__n_components': 20},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 2},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 5},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 8},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 12},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 18},\n",
       "  {'reduce_dim': SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 20}],\n",
       " 'split0_test_score': array([0.69806094, 0.82179132, 0.82640813, 0.83102493, 0.83471837,\n",
       "        0.83379501, 0.63804247, 0.64081256, 0.80609418, 0.83748846,\n",
       "        0.85226223, 0.85041551]),\n",
       " 'split1_test_score': array([0.69529086, 0.85964912, 0.86149584, 0.87626962, 0.86611265,\n",
       "        0.86795937, 0.64358264, 0.67313019, 0.81625115, 0.87257618,\n",
       "        0.88734995, 0.88734995]),\n",
       " 'split2_test_score': array([0.677747  , 0.83564174, 0.84764543, 0.84949215, 0.84764543,\n",
       "        0.84487535, 0.61588181, 0.66020314, 0.77100646, 0.84025854,\n",
       "        0.85595568, 0.85687904]),\n",
       " 'split3_test_score': array([0.68421053, 0.84025854, 0.84949215, 0.84579871, 0.84856879,\n",
       "        0.84579871, 0.62696214, 0.63342567, 0.79963066, 0.86611265,\n",
       "        0.87811634, 0.8790397 ]),\n",
       " 'split4_test_score': array([0.69593346, 0.85304991, 0.84658041, 0.84473198, 0.84750462,\n",
       "        0.84103512, 0.63031423, 0.65804067, 0.7948244 , 0.85859519,\n",
       "        0.8650647 , 0.86229205]),\n",
       " 'split5_test_score': array([0.67744917, 0.82162662, 0.83271719, 0.84658041, 0.84473198,\n",
       "        0.84195933, 0.61645102, 0.64602588, 0.78465804, 0.83641405,\n",
       "        0.85397412, 0.85212569]),\n",
       " 'split6_test_score': array([0.6987061 , 0.85397412, 0.85951941, 0.86783734, 0.87892791,\n",
       "        0.87985213, 0.6284658 , 0.64972274, 0.80683919, 0.87245841,\n",
       "        0.87892791, 0.88077634]),\n",
       " 'split7_test_score': array([0.70887246, 0.85027726, 0.84473198, 0.84658041, 0.85397412,\n",
       "        0.85489834, 0.60351201, 0.64695009, 0.79759704, 0.87060998,\n",
       "        0.88909427, 0.88632163]),\n",
       " 'split8_test_score': array([0.70332717, 0.84565619, 0.84473198, 0.84935305, 0.84380776,\n",
       "        0.84380776, 0.60813309, 0.64325323, 0.79667283, 0.86136784,\n",
       "        0.87892791, 0.86691312]),\n",
       " 'split9_test_score': array([0.70609982, 0.86691312, 0.86968577, 0.87338262, 0.87338262,\n",
       "        0.87338262, 0.62384473, 0.64602588, 0.8142329 , 0.87615527,\n",
       "        0.89279113, 0.89371534]),\n",
       " 'mean_test_score': array([0.69456763, 0.84488174, 0.84830007, 0.85310421, 0.8539357 ,\n",
       "        0.85273466, 0.6235218 , 0.64975979, 0.79878049, 0.85920177,\n",
       "        0.87324464, 0.87158167]),\n",
       " 'std_test_score': array([0.0106267 , 0.01436422, 0.01228719, 0.01374199, 0.01348059,\n",
       "        0.01482992, 0.01200615, 0.01072047, 0.01286827, 0.0147471 ,\n",
       "        0.01448487, 0.01498944]),\n",
       " 'rank_test_score': array([10,  8,  7,  5,  4,  6, 12, 11,  9,  3,  1,  2], dtype=int32),\n",
       " 'split0_train_score': array([0.74910173, 0.90144749, 0.91007083, 0.91797557, 0.92167129,\n",
       "        0.92177395, 0.62704034, 0.69592444, 0.83862026, 0.89426137,\n",
       "        0.92085002, 0.91417719]),\n",
       " 'split1_train_score': array([0.75310543, 0.90298737, 0.91007083, 0.92126065, 0.92095267,\n",
       "        0.92074736, 0.62642439, 0.65948055, 0.81952572, 0.89415871,\n",
       "        0.91766759, 0.91191869]),\n",
       " 'split2_train_score': array([0.74325018, 0.91284262, 0.91520378, 0.92403244, 0.92803614,\n",
       "        0.92865209, 0.62950416, 0.69202341, 0.82373473, 0.89795709,\n",
       "        0.91951545, 0.91746227]),\n",
       " 'split3_train_score': array([0.75259214, 0.89569859, 0.90401396, 0.91058413, 0.91972077,\n",
       "        0.91807823, 0.62827225, 0.66327892, 0.81973103, 0.88912843,\n",
       "        0.91787291, 0.91510112]),\n",
       " 'split4_train_score': array([0.74686923, 0.90546089, 0.91203038, 0.92116608, 0.92578526,\n",
       "        0.92588791, 0.62789982, 0.66054198, 0.84264011, 0.89755697,\n",
       "        0.91962636, 0.9173681 ]),\n",
       " 'split5_train_score': array([0.75097516, 0.9046397 , 0.91110655, 0.9205502 , 0.92270581,\n",
       "        0.9231164 , 0.62923424, 0.69800862, 0.81985219, 0.89858345,\n",
       "        0.92229522, 0.91552043]),\n",
       " 'split6_train_score': array([0.75354137, 0.90381852, 0.91038801, 0.91747075, 0.92239786,\n",
       "        0.92208992, 0.62810511, 0.6994457 , 0.81625949, 0.89150072,\n",
       "        0.91870253, 0.91510983]),\n",
       " 'split7_train_score': array([0.75035927, 0.90238144, 0.91008007, 0.91890782, 0.92178197,\n",
       "        0.92229522, 0.63077397, 0.66577705, 0.819031  , 0.89262985,\n",
       "        0.91972901, 0.91254363]),\n",
       " 'split8_train_score': array([0.75251488, 0.90422911, 0.91151714, 0.92003695, 0.92260316,\n",
       "        0.92137138, 0.61917471, 0.66136317, 0.83555738, 0.89642784,\n",
       "        0.91880517, 0.91562308]),\n",
       " 'split9_train_score': array([0.74810101, 0.90546089, 0.91254363, 0.91931842, 0.92578526,\n",
       "        0.92660645, 0.62861835, 0.70170396, 0.81728598, 0.89653049,\n",
       "        0.91808664, 0.91469924]),\n",
       " 'mean_train_score': array([0.75004104, 0.90389666, 0.91070252, 0.9191303 , 0.92314402,\n",
       "        0.92306189, 0.62750473, 0.67975478, 0.82522379, 0.89487349,\n",
       "        0.91931509, 0.91495236]),\n",
       " 'std_train_score': array([0.00309947, 0.00402395, 0.00268816, 0.003358  , 0.00243867,\n",
       "        0.00296727, 0.00301322, 0.01788837, 0.00930056, 0.00293148,\n",
       "        0.00135956, 0.00169189])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline grid search.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E8008)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Performance wise pipeline2 performed exceptionally well as compared to previous pipeline1. The accuracy came out to be 87.32%, which is really good. In my perspective, the data seems to be non-linearly seperable and since RandomForestClassifier works very well with non-linearly seperable data, it helped building a good decision boundary in the data which gave high accuracy. Here is thee f-score and confusion matrix:\n",
    "Confusion Matrix:===>  \n",
    " [[470  70]\n",
    " [ 92 705]] \n",
    "\n",
    "Classification Report:===>  \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.87      0.85       540\n",
    "           1       0.91      0.88      0.90       797\n",
    "\n",
    "   micro avg       0.88      0.88      0.88      1337\n",
    "   macro avg       0.87      0.88      0.87      1337\n",
    "weighted avg       0.88      0.88      0.88      1337\n",
    "\n",
    "Went_on_backorders are detected which is our success metric.\n",
    "\n",
    "\n",
    "Optimal hyperparameters:\n",
    "SelectKBest was chosen for the dimension reduction. SelectKbest method was set to f_regression and a variety of n_components were given as N_FEATURES_OPTIONS = [2,5,8,12,18,20] to be run. k=18 was selected as the optimal one. \n",
    "\n",
    "RandomForestClassifier was selected as the model with n_estimators=100, max_depth=10,random_state=1 as hyperparameters.\n",
    "\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', SelectKBest(k=18, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_im...mators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=1, verbose=0, warm_start=False))])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Red'>Your 3rd pipeline </font>\n",
    "  * Anomaly detection - <font color='Red'>EllipticEnvelope(support_fraction=1, contamination=0.2) method used</font>\n",
    "  * Dimensionality reduction - <font color='Red'>PCA() and selectKBest()</font>\n",
    "  * Model training/validation - <font color='Red'>LogisticRegression(solver='lbfgs')</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E8009)\n",
    "# ----------------------------------\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "envelope = EllipticEnvelope(support_fraction=1, contamination=0.2).fit(X_train)\n",
    "\n",
    "# Create an boolean indexing array to pick up outliers\n",
    "outliers = envelope.predict(X_train)==-1\n",
    "\n",
    "# Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "X_env = X_train[~outliers]\n",
    "y_env = y_train[~outliers]\n",
    "\n",
    "X_env2=replace_negative_values_with_0(X_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (12027, 22) X_env.shape: (9621, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \",X_train.shape, \"X_env.shape:\", X_env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=2,\n",
       "       param_grid=[{'reduce_dim': [PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)], 'reduce_dim__n_components': [5, 7, 8, 10, 12, 18], 'classifier__C': [1, 0.1, 3000.0]}, {'reduce_dim': [SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>)], 'reduce_dim__k': [5, 7, 8, 10, 12, 18], 'classifier__C': [1, 0.1, 3000.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pipeline with PCA and Random forest classifier\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "N_FEATURES_OPTIONS = [5,7,8,10,12,18]\n",
    "C_OPTIONS = [1, 1e-1, 3e3]\n",
    "param_grid = [{\n",
    "    'reduce_dim': [PCA(iterated_power=7)],\n",
    "    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "    'classifier__C': C_OPTIONS\n",
    "},\n",
    "    {\n",
    "        # A second set of tests cases for hyperparameters\n",
    "        'reduce_dim': [SelectKBest(f_regression)],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classifier__C': C_OPTIONS\n",
    "    },\n",
    "   \n",
    "]\n",
    "reducer_labels = ['PCA','KBest(chi2)'] #\n",
    "\n",
    "clf4 = GridSearchCV(pipe,cv=10, n_jobs=2, param_grid=param_grid)\n",
    "\n",
    "clf4.fit(X_env2, y_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[435 105]\n",
      " [117 680]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       540\n",
      "           1       0.87      0.85      0.86       797\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1337\n",
      "   macro avg       0.83      0.83      0.83      1337\n",
      "weighted avg       0.83      0.83      0.83      1337\n",
      "\n",
      "Best Accuracy Score:===>  0.825797734123272\n"
     ]
    }
   ],
   "source": [
    "y_pred4=clf4.predict(X_test2).round()\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test, y_pred4),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test, y_pred4))\n",
    "print(\"Best Accuracy Score:===> \", clf4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split5_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split6_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split7_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split8_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split9_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.6894145 , 1.75725484, 1.93365562, 2.40704446, 2.01250665,\n",
       "        1.50336368, 1.65684788, 1.66517582, 2.08171687, 1.9510772 ,\n",
       "        2.80622602, 1.58432474, 1.3049598 , 1.50047667, 1.73769503,\n",
       "        2.01901641, 1.83836453, 1.10613053, 0.53518987, 0.72288265,\n",
       "        0.92608991, 1.50344837, 1.48821666, 0.66174414, 0.50366209,\n",
       "        0.70473917, 0.95827558, 1.62211485, 1.50287652, 0.46263673,\n",
       "        0.34512308, 0.71867697, 0.8888417 , 1.26221528, 1.39413147,\n",
       "        0.59744401]),\n",
       " 'std_fit_time': array([0.21972775, 0.55980279, 0.390492  , 0.52453754, 0.29647712,\n",
       "        0.43140356, 0.47857213, 0.75381477, 0.3306847 , 0.58721538,\n",
       "        0.88456633, 0.64319114, 0.37981194, 0.45435315, 0.38329408,\n",
       "        0.35346463, 0.31477759, 0.26907455, 0.09704853, 0.24125366,\n",
       "        0.29550304, 0.19227707, 0.15652622, 0.28815497, 0.13232586,\n",
       "        0.20844806, 0.28831979, 0.17117804, 0.31251406, 0.16470846,\n",
       "        0.07248768, 0.18574868, 0.19003756, 0.30576889, 0.28138132,\n",
       "        0.38570239]),\n",
       " 'mean_score_time': array([0.00818307, 0.00712864, 0.00360219, 0.02080419, 0.01808968,\n",
       "        0.01942356, 0.01091025, 0.00363545, 0.01009507, 0.02979345,\n",
       "        0.01849561, 0.02576613, 0.00307498, 0.00295637, 0.0048641 ,\n",
       "        0.01070952, 0.01326938, 0.00919542, 0.00484648, 0.00543196,\n",
       "        0.00280051, 0.01975539, 0.0136148 , 0.01561725, 0.00300283,\n",
       "        0.00384839, 0.00214772, 0.0128242 , 0.00781429, 0.01000299,\n",
       "        0.00160491, 0.00552306, 0.0016916 , 0.00774624, 0.01090488,\n",
       "        0.00638812]),\n",
       " 'std_score_time': array([1.34107865e-02, 9.25362962e-03, 5.72670957e-03, 1.46541698e-02,\n",
       "        7.64521638e-03, 1.64512171e-02, 1.43397178e-02, 3.97966845e-03,\n",
       "        1.65916204e-02, 3.28367390e-02, 9.83109620e-03, 1.30258185e-02,\n",
       "        4.09099512e-03, 2.83779076e-03, 8.31627051e-03, 4.22580194e-03,\n",
       "        8.77221528e-03, 5.04832708e-03, 6.38225640e-03, 6.80188435e-03,\n",
       "        2.42447430e-03, 1.39223042e-02, 1.49592710e-02, 1.32172367e-02,\n",
       "        2.81180003e-03, 6.16404069e-03, 1.35391404e-03, 4.24633591e-03,\n",
       "        3.36013510e-03, 7.48638028e-03, 3.78290353e-05, 7.83714995e-03,\n",
       "        2.86736510e-04, 4.10317079e-03, 7.97799592e-03, 4.17097267e-03]),\n",
       " 'param_classifier__C': masked_array(data=[1, 1, 1, 1, 1, 1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 3000.0,\n",
       "                    3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 1, 1, 1, 1, 1,\n",
       "                    1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 3000.0, 3000.0,\n",
       "                    3000.0, 3000.0, 3000.0, 3000.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim': masked_array(data=[PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "                    SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim__n_components': masked_array(data=[5, 7, 8, 10, 12, 18, 5, 7, 8, 10, 12, 18, 5, 7, 8, 10,\n",
       "                    12, 18, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_reduce_dim__k': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
       "                    --, --, --, --, 5, 7, 8, 10, 12, 18, 5, 7, 8, 10, 12,\n",
       "                    18, 5, 7, 8, 10, 12, 18],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 5},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 7},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 8},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 10},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 12},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 18},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 5},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 7},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 8},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 10},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 12},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 18},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 5},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 7},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 8},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 10},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 12},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': PCA(copy=True, iterated_power=7, n_components=None, random_state=None,\n",
       "     svd_solver='auto', tol=0.0, whiten=False),\n",
       "   'reduce_dim__n_components': 18},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 5},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 7},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 8},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 10},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 12},\n",
       "  {'classifier__C': 1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 18},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 5},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 7},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 8},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 10},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 12},\n",
       "  {'classifier__C': 0.1,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 18},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 5},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 7},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 8},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 10},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 12},\n",
       "  {'classifier__C': 3000.0,\n",
       "   'reduce_dim': SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>),\n",
       "   'reduce_dim__k': 18}],\n",
       " 'split0_test_score': array([0.6728972 , 0.67393562, 0.67393562, 0.6728972 , 0.6728972 ,\n",
       "        0.6728972 , 0.6728972 , 0.67393562, 0.67393562, 0.6728972 ,\n",
       "        0.6728972 , 0.6728972 , 0.6728972 , 0.67393562, 0.67393562,\n",
       "        0.6728972 , 0.6728972 , 0.6728972 , 0.8172378 , 0.79543094,\n",
       "        0.8026999 , 0.80477674, 0.80477674, 0.75389408, 0.8172378 ,\n",
       "        0.79543094, 0.8026999 , 0.80477674, 0.80477674, 0.75389408,\n",
       "        0.8172378 , 0.79543094, 0.8026999 , 0.80477674, 0.80477674,\n",
       "        0.75389408]),\n",
       " 'split1_test_score': array([0.67185877, 0.67393562, 0.6728972 , 0.6728972 , 0.6728972 ,\n",
       "        0.6728972 , 0.67185877, 0.67393562, 0.6728972 , 0.6728972 ,\n",
       "        0.6728972 , 0.6728972 , 0.67185877, 0.67393562, 0.6728972 ,\n",
       "        0.6728972 , 0.6728972 , 0.6728972 , 0.83281412, 0.8463136 ,\n",
       "        0.84839045, 0.84942887, 0.84942887, 0.67912773, 0.83281412,\n",
       "        0.8463136 , 0.84735202, 0.84839045, 0.84942887, 0.67912773,\n",
       "        0.83281412, 0.8463136 , 0.84839045, 0.84942887, 0.84942887,\n",
       "        0.67912773]),\n",
       " 'split2_test_score': array([0.66458982, 0.66562825, 0.66874351, 0.66874351, 0.66874351,\n",
       "        0.66874351, 0.66458982, 0.66562825, 0.66874351, 0.66874351,\n",
       "        0.66874351, 0.66874351, 0.66458982, 0.66562825, 0.66874351,\n",
       "        0.66874351, 0.66874351, 0.66874351, 0.79439252, 0.81204569,\n",
       "        0.81204569, 0.8172378 , 0.8172378 , 0.77050883, 0.79439252,\n",
       "        0.81412253, 0.81204569, 0.8172378 , 0.8172378 , 0.77050883,\n",
       "        0.79439252, 0.81204569, 0.81204569, 0.8172378 , 0.8172378 ,\n",
       "        0.77050883]),\n",
       " 'split3_test_score': array([0.67497404, 0.68951194, 0.71754933, 0.72689512, 0.72793354,\n",
       "        0.72897196, 0.67497404, 0.70508827, 0.72897196, 0.72793354,\n",
       "        0.72793354, 0.77258567, 0.67497404, 0.78400831, 0.71754933,\n",
       "        0.7258567 , 0.79231568, 0.72897196, 0.81204569, 0.82658359,\n",
       "        0.82035306, 0.82139148, 0.82139148, 0.68224299, 0.81204569,\n",
       "        0.82658359, 0.82242991, 0.82242991, 0.82242991, 0.68224299,\n",
       "        0.81204569, 0.82658359, 0.82035306, 0.82139148, 0.82139148,\n",
       "        0.68224299]),\n",
       " 'split4_test_score': array([0.67879418, 0.68087318, 0.68191268, 0.68191268, 0.68087318,\n",
       "        0.68087318, 0.67879418, 0.68087318, 0.68191268, 0.68191268,\n",
       "        0.68087318, 0.68087318, 0.67879418, 0.68087318, 0.68191268,\n",
       "        0.68191268, 0.68087318, 0.68087318, 0.78274428, 0.80561331,\n",
       "        0.80873181, 0.80873181, 0.80873181, 0.58316008, 0.78274428,\n",
       "        0.80561331, 0.80769231, 0.80769231, 0.80769231, 0.58316008,\n",
       "        0.78274428, 0.80561331, 0.80873181, 0.80873181, 0.80665281,\n",
       "        0.58316008]),\n",
       " 'split5_test_score': array([0.67151767, 0.67255717, 0.67255717, 0.67151767, 0.68814969,\n",
       "        0.68814969, 0.67151767, 0.67255717, 0.67255717, 0.67151767,\n",
       "        0.76819127, 0.68814969, 0.67151767, 0.67255717, 0.67255717,\n",
       "        0.67151767, 0.77130977, 0.68814969, 0.77858628, 0.82120582,\n",
       "        0.82120582, 0.81704782, 0.81808732, 0.75779626, 0.77858628,\n",
       "        0.81808732, 0.82016632, 0.81808732, 0.81704782, 0.75779626,\n",
       "        0.77858628, 0.82120582, 0.82016632, 0.81600832, 0.81704782,\n",
       "        0.75779626]),\n",
       " 'split6_test_score': array([0.66424116, 0.67983368, 0.68191268, 0.68191268, 0.68087318,\n",
       "        0.75467775, 0.74220374, 0.67983368, 0.68191268, 0.68191268,\n",
       "        0.68087318, 0.79002079, 0.66424116, 0.67983368, 0.68191268,\n",
       "        0.68191268, 0.68087318, 0.78274428, 0.81912682, 0.82744283,\n",
       "        0.82744283, 0.82120582, 0.82016632, 0.8045738 , 0.81912682,\n",
       "        0.82744283, 0.82744283, 0.82120582, 0.82120582, 0.8045738 ,\n",
       "        0.81912682, 0.82744283, 0.82744283, 0.82120582, 0.82328482,\n",
       "        0.8045738 ]),\n",
       " 'split7_test_score': array([0.70655567, 0.70551509, 0.70863684, 0.70967742, 0.710718  ,\n",
       "        0.710718  , 0.70655567, 0.70967742, 0.70863684, 0.70967742,\n",
       "        0.710718  , 0.710718  , 0.70655567, 0.80228928, 0.70863684,\n",
       "        0.70967742, 0.710718  , 0.710718  , 0.82934443, 0.84391259,\n",
       "        0.84391259, 0.84391259, 0.84495317, 0.70135276, 0.82934443,\n",
       "        0.84391259, 0.84183143, 0.84391259, 0.84287201, 0.70135276,\n",
       "        0.82934443, 0.84391259, 0.84391259, 0.84391259, 0.84391259,\n",
       "        0.70135276]),\n",
       " 'split8_test_score': array([0.6867846 , 0.68990635, 0.68990635, 0.68990635, 0.68990635,\n",
       "        0.68990635, 0.6867846 , 0.68990635, 0.68990635, 0.68990635,\n",
       "        0.68990635, 0.68990635, 0.6867846 , 0.68990635, 0.68990635,\n",
       "        0.68990635, 0.68990635, 0.68990635, 0.80957336, 0.83766909,\n",
       "        0.83766909, 0.83870968, 0.83870968, 0.69406868, 0.80957336,\n",
       "        0.83662851, 0.83662851, 0.83870968, 0.83870968, 0.69406868,\n",
       "        0.80957336, 0.83662851, 0.83870968, 0.83870968, 0.83870968,\n",
       "        0.69406868]),\n",
       " 'split9_test_score': array([0.67950052, 0.71904266, 0.68158169, 0.77523413, 0.68158169,\n",
       "        0.68158169, 0.67950052, 0.81061394, 0.68158169, 0.77523413,\n",
       "        0.68158169, 0.68158169, 0.67950052, 0.71904266, 0.68158169,\n",
       "        0.79812695, 0.68158169, 0.68158169, 0.80437045, 0.83038502,\n",
       "        0.8314256 , 0.83454735, 0.83454735, 0.8012487 , 0.80437045,\n",
       "        0.83038502, 0.8314256 , 0.83350676, 0.83246618, 0.8251821 ,\n",
       "        0.80437045, 0.83038502, 0.8314256 , 0.83454735, 0.83454735,\n",
       "        0.8251821 ]),\n",
       " 'mean_test_score': array([0.67716454, 0.68506392, 0.68495998, 0.69514603, 0.68745453,\n",
       "        0.69493816, 0.68495998, 0.69618543, 0.68610332, 0.69524997,\n",
       "        0.69545785, 0.70283754, 0.67716454, 0.70418875, 0.68495998,\n",
       "        0.69732876, 0.70221391, 0.69774452, 0.80802411, 0.8246544 ,\n",
       "        0.82538198, 0.82569379, 0.82579773, 0.72279389, 0.80802411,\n",
       "        0.82444652, 0.82496622, 0.82558986, 0.82538198, 0.72518449,\n",
       "        0.80802411, 0.82455046, 0.82538198, 0.82558986, 0.82569379,\n",
       "        0.72518449]),\n",
       " 'std_test_score': array([0.0117122 , 0.01562548, 0.01536799, 0.0320175 , 0.0175581 ,\n",
       "        0.02661434, 0.02196982, 0.04045384, 0.01795685, 0.03212191,\n",
       "        0.02993259, 0.04099505, 0.0117122 , 0.0467671 , 0.01536799,\n",
       "        0.03784813, 0.04159451, 0.03338434, 0.01732281, 0.01562645,\n",
       "        0.01441298, 0.01433016, 0.01444264, 0.06452967, 0.01732281,\n",
       "        0.01548735, 0.01400195, 0.01414011, 0.01418367, 0.06775461,\n",
       "        0.01732281, 0.01554277, 0.01453865, 0.01439611, 0.01453974,\n",
       "        0.06775461]),\n",
       " 'rank_test_score': array([35, 31, 32, 27, 29, 28, 32, 24, 30, 26, 25, 20, 35, 19, 32, 23, 21,\n",
       "        22, 13, 10,  6,  2,  1, 18, 13, 12,  9,  4,  6, 16, 13, 11,  6,  4,\n",
       "         2, 16], dtype=int32),\n",
       " 'split0_train_score': array([0.67787018, 0.68098868, 0.68110418, 0.68202818, 0.68214368,\n",
       "        0.68214368, 0.67787018, 0.68098868, 0.68110418, 0.68202818,\n",
       "        0.68214368, 0.68214368, 0.67787018, 0.68098868, 0.68110418,\n",
       "        0.68202818, 0.68214368, 0.68214368, 0.84049434, 0.82917533,\n",
       "        0.83148533, 0.83206283, 0.83136983, 0.78193578, 0.84049434,\n",
       "        0.82905983, 0.83136983, 0.83171633, 0.83183183, 0.78193578,\n",
       "        0.84049434, 0.82917533, 0.83160083, 0.83206283, 0.83148533,\n",
       "        0.78193578]),\n",
       " 'split1_train_score': array([0.67971818, 0.68145068, 0.68145068, 0.68168168, 0.68168168,\n",
       "        0.68168168, 0.67971818, 0.68145068, 0.68145068, 0.68168168,\n",
       "        0.68168168, 0.68168168, 0.67971818, 0.68145068, 0.68145068,\n",
       "        0.68168168, 0.68168168, 0.68168168, 0.8039963 , 0.82305382,\n",
       "        0.82582583, 0.82594133, 0.82594133, 0.69022869, 0.8039963 ,\n",
       "        0.82305382, 0.82605683, 0.82628783, 0.82559483, 0.69022869,\n",
       "        0.8039963 , 0.82305382, 0.82582583, 0.82617233, 0.82582583,\n",
       "        0.69022869]),\n",
       " 'split2_train_score': array([0.68064218, 0.68098868, 0.68237468, 0.68260568, 0.68260568,\n",
       "        0.68272118, 0.68064218, 0.68098868, 0.68237468, 0.68260568,\n",
       "        0.68260568, 0.68272118, 0.68064218, 0.68098868, 0.68237468,\n",
       "        0.68260568, 0.68260568, 0.68272118, 0.80665281, 0.82836683,\n",
       "        0.82813583, 0.82986833, 0.83009933, 0.78078078, 0.80665281,\n",
       "        0.82802033, 0.82755833, 0.82998383, 0.82986833, 0.78078078,\n",
       "        0.80665281, 0.82848233, 0.82813583, 0.82986833, 0.82825133,\n",
       "        0.78078078]),\n",
       " 'split3_train_score': array([0.68006468, 0.69392469, 0.72684223, 0.73689074, 0.74093324,\n",
       "        0.74162624, 0.68006468, 0.71656272, 0.73481173, 0.73746824,\n",
       "        0.74093324, 0.77223377, 0.68006468, 0.79267729, 0.72661123,\n",
       "        0.73689074, 0.78944329, 0.74162624, 0.8016863 , 0.82189882,\n",
       "        0.82594133, 0.82617233, 0.82605683, 0.69161469, 0.8016863 ,\n",
       "        0.82178332, 0.82582583, 0.82640333, 0.82663433, 0.69161469,\n",
       "        0.8016863 , 0.82201432, 0.82582583, 0.82617233, 0.82617233,\n",
       "        0.69161469]),\n",
       " 'split4_train_score': array([0.67617508, 0.67906225, 0.67917773, 0.67894676, 0.67929322,\n",
       "        0.67940871, 0.67617508, 0.67906225, 0.67917773, 0.67894676,\n",
       "        0.67929322, 0.67940871, 0.67617508, 0.67906225, 0.67917773,\n",
       "        0.67894676, 0.67929322, 0.67940871, 0.80471186, 0.82503753,\n",
       "        0.8279247 , 0.82815568, 0.82815568, 0.58355468, 0.80471186,\n",
       "        0.82503753, 0.82815568, 0.82769373, 0.82769373, 0.58355468,\n",
       "        0.80471186, 0.82503753, 0.8279247 , 0.82815568, 0.82757824,\n",
       "        0.58355468]),\n",
       " 'split5_train_score': array([0.68275782, 0.68529853, 0.68610694, 0.68587597, 0.70285252,\n",
       "        0.70262155, 0.68275782, 0.68529853, 0.68610694, 0.68587597,\n",
       "        0.79327867, 0.70262155, 0.68275782, 0.68529853, 0.68610694,\n",
       "        0.68587597, 0.79708973, 0.70262155, 0.80829195, 0.82665435,\n",
       "        0.82676983, 0.82838665, 0.82780922, 0.77872734, 0.80829195,\n",
       "        0.82653886, 0.82665435, 0.82838665, 0.82827116, 0.77872734,\n",
       "        0.80829195, 0.82665435, 0.82665435, 0.8288486 , 0.82757824,\n",
       "        0.77872734]),\n",
       " 'split6_train_score': array([0.6646264 , 0.67871579, 0.67940871, 0.67975517, 0.67987065,\n",
       "        0.76775609, 0.74766139, 0.67871579, 0.67940871, 0.67975517,\n",
       "        0.67987065, 0.78889017, 0.6646264 , 0.67871579, 0.67940871,\n",
       "        0.67975517, 0.67987065, 0.79062247, 0.80159372, 0.82434461,\n",
       "        0.82503753, 0.82630789, 0.82688532, 0.77607114, 0.80159372,\n",
       "        0.82526851, 0.82515302, 0.82653886, 0.82642337, 0.77607114,\n",
       "        0.80159372, 0.8244601 , 0.82503753, 0.82630789, 0.82688532,\n",
       "        0.77607114]),\n",
       " 'split7_train_score': array([0.67817552, 0.69064665, 0.67886836, 0.67944573, 0.67944573,\n",
       "        0.67933025, 0.67817552, 0.67898383, 0.67886836, 0.67944573,\n",
       "        0.67944573, 0.67933025, 0.67817552, 0.78787529, 0.67886836,\n",
       "        0.67944573, 0.67944573, 0.67933025, 0.80103926, 0.82401848,\n",
       "        0.823903  , 0.82517321, 0.82505774, 0.68729792, 0.80103926,\n",
       "        0.82367206, 0.82378753, 0.82528868, 0.8256351 , 0.68729792,\n",
       "        0.80103926, 0.823903  , 0.82401848, 0.82528868, 0.82505774,\n",
       "        0.68729792]),\n",
       " 'split8_train_score': array([0.67771363, 0.67921478, 0.67921478, 0.67990762, 0.68002309,\n",
       "        0.68002309, 0.67771363, 0.67921478, 0.67921478, 0.67990762,\n",
       "        0.68002309, 0.68002309, 0.67771363, 0.67921478, 0.67921478,\n",
       "        0.67990762, 0.68002309, 0.68002309, 0.80484988, 0.82551963,\n",
       "        0.82586605, 0.82702079, 0.82632794, 0.69006928, 0.80484988,\n",
       "        0.82505774, 0.82517321, 0.82690531, 0.82690531, 0.69006928,\n",
       "        0.80484988, 0.82575058, 0.82551963, 0.82713626, 0.82736721,\n",
       "        0.69006928]),\n",
       " 'split9_train_score': array([0.67378753, 0.70115473, 0.67575058, 0.7448037 , 0.67598152,\n",
       "        0.676097  , 0.67378753, 0.78579677, 0.67575058, 0.74491917,\n",
       "        0.67598152, 0.676097  , 0.67378753, 0.70127021, 0.67575058,\n",
       "        0.78337182, 0.67598152, 0.676097  , 0.80277136, 0.82401848,\n",
       "        0.8243649 , 0.82494226, 0.82471132, 0.8095843 , 0.80277136,\n",
       "        0.82413395, 0.82424942, 0.82494226, 0.82471132, 0.81789838,\n",
       "        0.80277136, 0.8243649 , 0.82448037, 0.82494226, 0.82482679,\n",
       "        0.81778291]),\n",
       " 'mean_train_score': array([0.67715312, 0.68514455, 0.68502989, 0.69319412, 0.6884831 ,\n",
       "        0.69734095, 0.68545662, 0.69470627, 0.68582684, 0.69326342,\n",
       "        0.69752572, 0.70251511, 0.67715312, 0.70475422, 0.68500679,\n",
       "        0.69705094, 0.70275783, 0.69962759, 0.80760878, 0.82520879,\n",
       "        0.82652543, 0.82740313, 0.82724145, 0.72698646, 0.80760878,\n",
       "        0.82516259, 0.8263984 , 0.82741468, 0.82735693, 0.72781787,\n",
       "        0.80760878, 0.82528963, 0.82650234, 0.82749552, 0.82710284,\n",
       "        0.72780632]),\n",
       " 'std_train_score': array([0.0047929 , 0.00724835, 0.01417193, 0.02396867, 0.01883256,\n",
       "        0.03005734, 0.020868  , 0.03225423, 0.0165289 , 0.02409908,\n",
       "        0.0367486 , 0.03979627, 0.0047929 , 0.04323568, 0.01410378,\n",
       "        0.03326363, 0.0453209 , 0.03567254, 0.01117904, 0.00217031,\n",
       "        0.00210324, 0.00212954, 0.00203947, 0.06628357, 0.01117904,\n",
       "        0.00210792, 0.00210108, 0.00200198, 0.00204811, 0.06735783,\n",
       "        0.01117904, 0.00215923, 0.00212117, 0.00212574, 0.00181532,\n",
       "        0.0673424 ])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline grid search.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E8010)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Performance wise pipeline3 performed good. It definitely outperformed pipeline 1 which has a very low accuracy.  The accuracy came out to be 82.579%, which is little less than what we achieved with pipeline 2. Here is the f -score and confusion matrix:\n",
    "Confusion Matrix:===>  \n",
    " [[435 105]\n",
    " [117 680]] \n",
    "\n",
    "Classification Report:===>  \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      0.81      0.80       540\n",
    "           1       0.87      0.85      0.86       797\n",
    "\n",
    "   micro avg       0.83      0.83      0.83      1337\n",
    "   macro avg       0.83      0.83      0.83      1337\n",
    "weighted avg       0.83      0.83      0.83      1337\n",
    "\n",
    "Went_on_backorders are detected which is our sucess metric.\n",
    "\n",
    "Optimal hyperparameters:\n",
    "SelectKBest was chosen for the dimension reduction. SelectKbest method was set to f_regression and a variety of n_components were given as N_FEATURES_OPTIONS = [5,7,8,10,12,18] to be run. k=12 was selected as the optimal one. There were total 36 models built up using the grid hyperparameters and the 35th models gave the best results with accuracy mean accuracy as 82.57%. \n",
    "\n",
    "Logistic Regression was the classifier with solver='lbfgs' and C=1 was chosen to be the optimal hyperparameter.\n",
    "\n",
    "\n",
    "Best Model:>\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', SelectKBest(k=12, score_func=<function f_regression at 0x7f1b5d780c80>)), ('classifier', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
    "          tol=0.0001, verbose=0, warm_start=False))])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document the cross-validation analysis for the three models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your analysis in this cell (Question #E8011)\n",
    "# ----------------------------------\n",
    "\n",
    "Somehow I felt that the mean test scores and the rank test score did not comply with each other, but stil here are the results from cross validation.\n",
    "\n",
    " ==>> Pipeline 1: There were 14 models run with cv=10. As per the rank test score, 5th model gave the best result. Here are the mean test results and their rank of the models:\n",
    " \n",
    " 'mean_test_score': array([-1.14134214e-04, -2.29830305e-02, -6.96905707e-02, -6.72861379e-02,\n",
    "        -2.19819857e+00, -2.14493927e+00, -1.80681961e+00,  3.69513125e-02,\n",
    "         4.16218841e-02,  4.13706266e-02,  4.21428450e-02, -2.00401838e-03,\n",
    "        -5.12950024e-03, -1.45212747e+00]),\n",
    "'rank_test_score': array([ 5,  8, 10,  9, 14, 13, 12,  4,  2,  3,  1,  6,  7, 11],\n",
    "       dtype=int32),\n",
    "       \n",
    " ==>> Pipeline 2: There were 12 models run with cv=10. As per the rank test score, 10th model gave the best result. Here are the mean test results and their rank of the models:     \n",
    " 'mean_test_score': array([0.69456763, 0.84488174, 0.84830007, 0.85310421, 0.8539357 ,\n",
    "        0.85273466, 0.6235218 , 0.64975979, 0.79878049, 0.85920177,\n",
    "        0.87324464, 0.87158167]),\n",
    "        \n",
    "'rank_test_score': array([10,  8,  7,  5,  4,  6, 12, 11,  9,  3,  1,  2], dtype=int32),\n",
    "\n",
    "\n",
    "==>> Pipeline 3 : There were 36 models and as per the rank test score, 35th model gave the best result.\n",
    "'mean_test_score': array([0.67716454, 0.68506392, 0.68495998, 0.69514603, 0.68745453,\n",
    "        0.69493816, 0.68495998, 0.69618543, 0.68610332, 0.69524997,\n",
    "        0.69545785, 0.70283754, 0.67716454, 0.70418875, 0.68495998,\n",
    "        0.69732876, 0.70221391, 0.69774452, 0.80802411, 0.8246544 ,\n",
    "        0.82538198, 0.82569379, 0.82579773, 0.72279389, 0.80802411,\n",
    "        0.82444652, 0.82496622, 0.82558986, 0.82538198, 0.72518449,\n",
    "        0.80802411, 0.82455046, 0.82538198, 0.82558986, 0.82569379,\n",
    "        0.72518449]),\n",
    "'rank_test_score': array([35, 31, 32, 27, 29, 28, 32, 24, 30, 26, 25, 20, 35, 19, 32, 23, 21,\n",
    "        22, 13, 10,  6,  2,  1, 18, 13, 12,  9,  4,  6, 16, 13, 11,  6,  4,\n",
    "         2, 16], dtype=int32),\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may want to pickle some models that do some things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a suggestion :)\n",
    "# ----------------------------\n",
    "\n",
    "Logistic Regression and Random Forest Classifiers performed well over Ridge, so I would like to pick them \n",
    "on the final training and predicting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a few commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Pipelines done`\n",
    "\n",
    "### <center><span style='color:green'>This becomes the new Starting Point after pipeline grid search work</span></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retrain a model using the full training data set\n",
    "\n",
    "## Train\n",
    "Use the full training data set to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8012)\n",
    "# ----------------------------------\n",
    "\n",
    "X_whole=dataset.iloc[:,:-1]\n",
    "y_whole=dataset.went_on_backorder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "X_whole2=replace_negative_values_with_0(X_whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the complete training data into ratio of 90% training and 10% validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_whole2, y_whole, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[     2   1100]\n",
      " [     2 157593]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      1102\n",
      "           1       0.99      1.00      1.00    157595\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    158697\n",
      "   macro avg       0.75      0.50      0.50    158697\n",
      "weighted avg       0.99      0.99      0.99    158697\n",
      "\n",
      "Best Accuracy Score:===>  0.9930559493878272\n"
     ]
    }
   ],
   "source": [
    "classifier=LogisticRegression(solver='lbfgs', C=0.1, n_jobs=2)\n",
    "classifier.fit(X_train1,y_train1)\n",
    "y_pred=classifier.predict(X_test1)\n",
    "\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test1, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test1, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test1, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model borrowed from 3rd pipeline failed terribly with imbalanced full training data, yielding 0 went_backorders, as we can see from the confusion matrix, even though accuracy was pretty good.\n",
    "\n",
    "#####  3rd pipeline was trained and tested on smartly re-sampled balanced data. But now we are dealing with the total training data which is highly unbalanced; and to deal with unbalanced data like this I added one more parameter in the classifier called \"class_weight\", which improved the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight={1: 5, 0: 180}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=2, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight=dict({1:5, 0:180}) #1:3 - 98%\n",
    "classifier=LogisticRegression(solver='lbfgs', C=0.1, n_jobs=2,class_weight=class_weight)\n",
    "classifier.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modification with the class_weight helped predicting went_backorders, which was our sucess metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression performace test on the  Validation data (10 % of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[    37   1074]\n",
      " [   194 157392]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.03      0.06      1111\n",
      "           1       0.99      1.00      1.00    157586\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    158697\n",
      "   macro avg       0.58      0.52      0.53    158697\n",
      "weighted avg       0.99      0.99      0.99    158697\n",
      "\n",
      "Best Accuracy Score:===>  0.9920099308745597\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier.predict(X_test1)\n",
    "\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test1, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test1, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similarly, RandomForestClassifier performed badly without class_weight and I tried and tested it with both (without and with class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=2,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2= RandomForestClassifier(n_estimators=100, max_depth=10,random_state=0, n_jobs=2)\n",
    "classifier2.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[     2   1109]\n",
      " [     0 157586]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1111\n",
      "           1       0.99      1.00      1.00    157586\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    158697\n",
      "   macro avg       1.00      0.50      0.50    158697\n",
      "weighted avg       0.99      0.99      0.99    158697\n",
      "\n",
      "Best Accuracy Score:===>  0.9930118401734123\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier2.predict(X_test1)\n",
    "\n",
    "#y_pred4=clf4.predict(X_test2).round()\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test1, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test1, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={1: 1, 0: 20},\n",
       "            criterion='gini', max_depth=10, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=2, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight=dict({1:1, 0:20}) #1:3 - 98%\n",
    "classifier2= RandomForestClassifier(n_estimators=100, max_depth=10,random_state=0, n_jobs=2, class_weight=class_weight)\n",
    "classifier2.fit(X_train1,y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier performace test on the  Validation data (10 % of training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[   500    602]\n",
      " [  2274 155321]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.45      0.26      1102\n",
      "           1       1.00      0.99      0.99    157595\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    158697\n",
      "   macro avg       0.59      0.72      0.62    158697\n",
      "weighted avg       0.99      0.98      0.99    158697\n",
      "\n",
      "Best Accuracy Score:===>  0.9818774141918247\n"
     ]
    }
   ],
   "source": [
    "y_pred=classifier2.predict(X_test1)\n",
    "\n",
    "#y_pred4=clf4.predict(X_test2).round()\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test1, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test1, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library.\n",
    "Saved two models with class_weight parameter, which proved to be a game changer in case of the unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForest2.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E8013)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "#joblib.dump(classifier, 'LogisticRegression.pkl')\n",
    "\n",
    "joblib.dump(classifier2, 'RandomForest2.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the trained model from the pickle file\n",
    "### Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E8014)\n",
    "# ----------------------------------\n",
    "loaded_model = joblib.load('LogisticRegression.pkl')\n",
    "\n",
    "loaded_model2 = joblib.load('RandomForest2.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test your new model using the testing data set.\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and cleaning of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>sales_9_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>205.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242071</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242072</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242074</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242075</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227351 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0               10.0        2.0             0.0               0.0   \n",
       "1               41.0        8.0             0.0               0.0   \n",
       "2                9.0        4.0             1.0              10.0   \n",
       "3              205.0        8.0           123.0             277.0   \n",
       "4               47.0        2.0             0.0               0.0   \n",
       "...              ...        ...             ...               ...   \n",
       "242071           4.0        8.0             0.0               0.0   \n",
       "242072          10.0       12.0             0.0               0.0   \n",
       "242073           9.0        8.0             0.0               0.0   \n",
       "242074          76.0        9.0             0.0              64.0   \n",
       "242075          46.0        4.0             0.0               2.0   \n",
       "\n",
       "        forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0                    0.0               0.0            0.0            0.0   \n",
       "1                    0.0               0.0            0.0            0.0   \n",
       "2                   19.0              25.0            1.0            6.0   \n",
       "3                  597.0             917.0           92.0          311.0   \n",
       "4                    0.0               0.0            1.0            2.0   \n",
       "...                  ...               ...            ...            ...   \n",
       "242071               1.0               2.0            0.0            0.0   \n",
       "242072               0.0               0.0            0.0            0.0   \n",
       "242073               0.0               0.0            0.0            0.0   \n",
       "242074              96.0              96.0            0.0            0.0   \n",
       "242075               2.0               2.0            0.0            1.0   \n",
       "\n",
       "        sales_6_month  sales_9_month  ...  pieces_past_due  perf_6_month_avg  \\\n",
       "0                 0.0            0.0  ...              0.0              0.33   \n",
       "1                 1.0            1.0  ...              0.0              0.57   \n",
       "2                20.0           24.0  ...              0.0              0.73   \n",
       "3               637.0          908.0  ...              0.0            -99.00   \n",
       "4                 5.0            9.0  ...              0.0              0.80   \n",
       "...               ...            ...  ...              ...               ...   \n",
       "242071            0.0            0.0  ...              0.0              0.35   \n",
       "242072            0.0            0.0  ...              0.0              0.48   \n",
       "242073            0.0            0.0  ...              0.0              0.99   \n",
       "242074            0.0            0.0  ...             32.0              0.90   \n",
       "242075            2.0            4.0  ...              0.0              0.56   \n",
       "\n",
       "        perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
       "0                    0.24           0.0        0.0            1.0        1.0   \n",
       "1                    0.42           0.0        0.0            1.0        0.0   \n",
       "2                    0.78           0.0        1.0            1.0        1.0   \n",
       "3                  -99.00           0.0        1.0            1.0        0.0   \n",
       "4                    0.75           0.0        1.0            1.0        1.0   \n",
       "...                   ...           ...        ...            ...        ...   \n",
       "242071               0.36           0.0        1.0            1.0        1.0   \n",
       "242072               0.48           0.0        0.0            1.0        1.0   \n",
       "242073               0.99           0.0        1.0            1.0        0.0   \n",
       "242074               0.88           0.0        1.0            1.0        1.0   \n",
       "242075               0.58           0.0        1.0            1.0        1.0   \n",
       "\n",
       "        stop_auto_buy  rev_stop  went_on_backorder  \n",
       "0                 0.0       1.0                1.0  \n",
       "1                 0.0       1.0                1.0  \n",
       "2                 0.0       1.0                1.0  \n",
       "3                 0.0       1.0                1.0  \n",
       "4                 0.0       1.0                1.0  \n",
       "...               ...       ...                ...  \n",
       "242071            0.0       1.0                1.0  \n",
       "242072            0.0       1.0                1.0  \n",
       "242073            0.0       1.0                1.0  \n",
       "242074            0.0       1.0                1.0  \n",
       "242075            0.0       1.0                1.0  \n",
       "\n",
       "[227351 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Add code below this comment  (Question #E8015)\n",
    "# ----------------------------------\n",
    "\n",
    "## Data cleaning\n",
    "\n",
    "TEST='/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(TEST)\n",
    "test=pd.read_csv(TEST).sample(frac = 1).reset_index(drop=True)\n",
    "test=test.drop('sku', axis=1)\n",
    "yes_no_columns_test = list(filter(lambda i: test[i].dtype!=np.float64, test.columns))\n",
    "for column_name in yes_no_columns_test:\n",
    "    mode = test[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    test[column_name].fillna(mode, inplace=True)\n",
    "\n",
    "CovertToBinary(yes_no_columns_test,test)\n",
    "clean_dataset(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the test data into X data and y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "X_test_full=test.iloc[:,:-1]\n",
    "y_test_full=test.went_on_backorder\n",
    "X_test_full2=replace_negative_values_with_0(X_test_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions made with pickled LOGISTIC REGRESSION model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[    53   2551]\n",
      " [   302 224445]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.02      0.04      2604\n",
      "           1       0.99      1.00      0.99    224747\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    227351\n",
      "   macro avg       0.57      0.51      0.51    227351\n",
      "weighted avg       0.98      0.99      0.98    227351\n",
      "\n",
      "Best Accuracy Score:===>  0.9874511218336405\n"
     ]
    }
   ],
   "source": [
    "##Logistic Regression\n",
    "y_pred=loaded_model.predict(X_test_full2)\n",
    "\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test_full, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test_full, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test_full, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions made with pickled Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:===>  \n",
      " [[   932   1672]\n",
      " [  2881 221866]] \n",
      "\n",
      "Classification Report:===>  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.36      0.29      2604\n",
      "           1       0.99      0.99      0.99    224747\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    227351\n",
      "   macro avg       0.62      0.67      0.64    227351\n",
      "weighted avg       0.98      0.98      0.98    227351\n",
      "\n",
      "Best Accuracy Score:===>  0.9799736970587329\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "y_pred=loaded_model2.predict(X_test_full2)\n",
    "\n",
    "print(\"Confusion Matrix:===> \",\"\\n\", confusion_matrix(y_test_full, y_pred),\"\\n\")\n",
    "print(\"Classification Report:===> \",\"\\n\", classification_report(y_test_full, y_pred))\n",
    "print(\"Best Accuracy Score:===> \", accuracy_score(y_test_full, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E8016)\n",
    "# ----------------------------------\n",
    "\n",
    "Data provided was very messy. It had NAN,long float values and negative values, which required me to write data cleaning methods. Without cleaning when I tried to give this data to the model, the model was constantly failing which made me realise how important data cleaning was, especially in the data like this. \n",
    "\n",
    "Next step was the smart sampling. Since the data was quite big and unbalanced, there was a need of smartly sampling the balanced data. And I sampled total 13364 data points out of 1586967 with the ratio of went_on_backorder ratio: 0.5936845255911404, where in total training set went_on_backorder ratio: 0.993088073034915. This shows how imbalanced the data is.\n",
    "\n",
    "Next I split this subset into test and train to test each pipeline with the confusion matrix and f-score. Because our main metric was to detect went_backorders; and confusion matrix would have given correct quantification for that. I used three classifiers, Ridge, Logistic Regression and RandomForestClassifier. All the results are mentioned above. But these three models, gave me a sense that the data is non-linealy seperable and that is why Ridge() classifier did not give very good results as compared to the other two.\n",
    "\n",
    "Now when the models were trained and tested using grid search and cross validation=10, I got the best models for Logistic Regression(~87% accuracy) and Random Forest Classifier(~82% accuracy). \n",
    "\n",
    "Considering the huge imbalance in data, model performance on the full training data did not came out to be as good as on the subset data; because we had trained the models on balanced subset and now when we were trying to train the same models using the whole training data which was unbalanced. As a result, none of the went_backorders were getting detected even though the accuracy was 99%. So, I made a modification by adding a new parameter class_weight to Logistic Regression and Random Forest classifier. This was a game changer.\n",
    "\n",
    "With class_weight feature, both the models perfomed very well. Here are the results from Random Forest classifier and Logistic regression classifier on the full test set.\n",
    "\n",
    "===> From Random Forest.\n",
    "Confusion Matrix:===>  \n",
    " [[  1943    661]\n",
    " [ 22029 202718]] \n",
    "\n",
    "Classification Report:===>  \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.08      0.75      0.15      2604\n",
    "           1       1.00      0.90      0.95    224747\n",
    "\n",
    "   micro avg       0.90      0.90      0.90    227351\n",
    "   macro avg       0.54      0.82      0.55    227351\n",
    "weighted avg       0.99      0.90      0.94    227351\n",
    "\n",
    "Best Accuracy Score:===>  0.9001983716807931\n",
    "\n",
    "===> From Logistic Regression:\n",
    "Confusion Matrix:===>  \n",
    " [[    53   2551]\n",
    " [   302 224445]] \n",
    "\n",
    "Classification Report:===>  \n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.15      0.02      0.04      2604\n",
    "           1       0.99      1.00      0.99    224747\n",
    "\n",
    "   micro avg       0.99      0.99      0.99    227351\n",
    "   macro avg       0.57      0.51      0.51    227351\n",
    "weighted avg       0.98      0.99      0.98    227351\n",
    "\n",
    "Best Accuracy Score:===>  0.9874511218336405\n",
    "\n",
    "\n",
    "In my peerspective, Random Forest performed better than Logistic Regression, even though latter gave the best accuracy of 98.75%. However, random forest helped us predicting more went_backorders correctly, if we see the confusion matrix above, even though accuracy was 90%. And our main aim was to predict went_backorders correctly and predictions made were correct as we can see from the confusion matrix. \n",
    "\n",
    "Predicting  'went_backorders' was our \"success\" metric and in this case my models were quite sucessful in that. Our success metric was not overweighted by the data imbalance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E8017)\n",
    "# ----------------------------------\n",
    "\n",
    "I discovered that the data was highly imbalanced and only 00.70% of the cases were went_backorder instances and the rest 99.30% were without backorders. For a data like this, yes, I do recommend my RandomForestClassifier for operationalizing the business. It might not have the perfect accuracy; but it had an accuracy of 90%, with a good number of correct predictions made for went_backorder instances. And this was our sucess metric. So based on the predictive model, I am confident that it could predict \"SOME\" backorders instances if not all. This might help company to save some of the money atleast. So, I think this model can be used in the supply chain business model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
